---
title: "Banking Capstone - Machine Learning Applied"
author: "Demetri Lee"
date: "April 1, 2019"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(caret)
knitr::opts_chunk$set(echo = TRUE)
```

## Trying to predict the success of bank telemarketing

I'm working on predicting the success of telemarketing calls for a bank that's 
selling long-term deposits. A Portuguese retail bank was addressed, with data 
collected from 2008 to 2013, thus including the effects of the recent 
financial crisis of 2008. With 21 features available for analysis, I hope to 
derive knowledge for a model that would confirm how to better focus resources 
towards clients with a high chance of agreeing to registering for extra 
banking services. 

The [data set](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing) can be 
found at UC Irvine's archive for Machine Learning.

```{r}
bankfull_clean <- read_csv("bankfull_clean.csv", col_types = "ifffffffffiiiifdddddf")
```


I will be treating this as a supervised classification problem. While 
predicting whether a customer will subscribe to a new service, I need to find 
the main predictors while developing a model that best expressses a client's 
decision for subscribing. 

## Set Up Model Parameters

Let's start with partitioning the dataset into a training, validation, and 
test set.

1. Split the full dataset into 80% and 20% partitions
2. Split the main 80% into a Training (75%) and Validation (25%) set
3. Original 20% kept aside as Test Data

```{r partition}
set.seed(13456)

TrainingValidationIndex <- createDataPartition(y=bankfull_clean$subscribed, p=0.80, list=FALSE)
training_validation <- bankfull_clean[TrainingValidationIndex,]

set.seed(13456)

trainIndex <- createDataPartition(training_validation$subscribed, p=0.75, list=FALSE)
training <- training_validation[trainIndex,]
validation <- training_validation[-trainIndex,]

testData  <- bankfull_clean[-TrainingValidationIndex,]

control <- trainControl(method = "cv", number = 2, classProbs = TRUE, verboseIter = TRUE)
```


## Predictive Modeling Options

Because this problem deals with binary classification, I will be using these 2 
model types:

* Logistic Regression
    * regression model used to predict a binary dependent variable 
    
* Random Forest
    * an ensemble of decision trees analyzed for the best prediction 
    

## Logistic Regression with caret package

Develop a logistic regression model with the training data. 

```{r logReg, warning=FALSE}
# develop a model
set.seed(7)
glm_model <- train(subscribed ~ . - duration_sec, data=training, method="glm", na.action=na.omit, 
                   metric="Kappa", trControl=control)
```


What can we observe from the logistic regression model? 

```{r}
summary(glm_model) # results with entire variable set less the duration of last call
```


Let's apply our logistic regression model to the validation set.

```{r}
glm_predict <- predict(glm_model, newdata=validation)
```


While reviewing the confusion matrix for the logistic regression model, I will 
use the Kappa value for evaluation. This is because this data set is 
asymmetrical severely imbalanced so the accuracy value is impractical.

```{r}
caret::confusionMatrix(glm_predict, validation$subscribed, mode="everything", positive="yes") 
```

**Comparing results of Logistic Regression model:**

Kappa = 0.2976


## Random Forest with caret package

Develop a model with training data .

```{r rdmForest, warning=FALSE}
set.seed(7)
rf_model <- caret::train(subscribed~.-duration_sec, data=training, method="rf", metric="Kappa", 
                         trControl=control)
```


Apply the Random Forest model to the Validation set.

```{r validationData}
rf_prediction <- predict(rf_model, newdata=validation)
```


Review the Confusion Matrix for the Random Forest model, particularly taking  
note of the Kappa value.

```{r}
caret::confusionMatrix(rf_prediction, validation$subscribed, mode="everything", positive="yes")
```


**Performance of Random Forest**

Kappa = 0.3308

Because the Kappa value for the Random Forest model is higher than the Logistic 
Regression model, let's choose the Random Forest model for the final prediction 
with the test data.

```{r testData}
predict_testData <- predict(rf_model, newdata=testData)
caret::confusionMatrix(predict_testData, testData$subscribed, mode="everything", positive="yes")
```


**Confirming against Test Data**

Kappa = 0.3194

The Kappa score is lower with the test data than the training or validation 
set. With the test data, the positive predictive value claims this model will 
be correct 54% of the time and incorrect 46% of the time.

From exploratory data analysis, the rejection rate for subscriptions was 89%, 
meaning only 11% subscribe. The previous campaign had a rejection rate of 76%
and 24% acceptance.

While not impressive, having a 54% probability rate for subscriptions is 
better than the baseline of 11-24%.


**Recommendation**

The Random Forest model is a better predictor for this bank marketing dataset. 
However, there aren't any tangible results that can be applied. For better 
explanantory analysis I would recommend using the results from the Logistic 
Regression model. 
